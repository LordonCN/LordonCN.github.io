---
layout: post
title: "深度学习初探"
subtitle: 'PG，PG_DQN,DDPG'
author: "Lordon"
header-img: img/jassica/jessica-jung-celebrity.jpg
catalog: true
tags:
  - PG
  - Tensorflow
---
# 0、总体流程
> Q-learning与DQN

通过当前`observation`与下一步`obs_`获得两个预测值`V`来选择`Action`，当然这个缺点很明显：可视野很小。<br>
> Policy Gradients

通过不断地对`每一回合`各参数的的决策记录来进行`复盘`学习，当然这样效果是显而易见的，运算速度相对来说会变得慢。<br>
所以呢，两种方式结合一下得到类似于Actor-Critic的网络结构，把actor作为`执行机构`，critic作为`判断机构`能够给予执行机构指引，一个似眼，一个像腿。
<img src="/img/191026image/ddpgliuchengtu.jpg" >

# 1、DQN
QLearning是reninforcement learning中value-based的方法。Q即为Q（s,a），是在某一时刻的 `s`tate 下(s∈S)，采取 动作`a`ction (a∈A)获得收益的期望，环境会根据agent的动作反馈相应的回报`r`eward，所以算法的主要思想就是将`State`与`Action`构建成一张Q-table来存储Q值，然后根据Q值来选取能够获得最大的收益的动作，这就是贪婪策略。<br>
可描述如下：
```
1、初始化状态 s,alpha,gamma,Q空表
2、进入 episode 循环
3、基于状态s-->根据Q或者随机选择一个动作 a
4、执行动作a, 从环境获得 奖励r和下一个状态 s_
5、更新Q值： Q(s,a)= Q(s,a)+alpha*loss , loss=r+gamma*maxQ(s',a')-Q(s,a)
6、s = s_
7、若 episode 未结束，回到 2
```

# 2、Policy Gradients
策略梯度决策(PG)是一种不通过分析每一步动作a所得到的回报r来的策略，简单来说，在走迷宫的游戏中，PG对一局游戏的连续区间所执行的动作a进行学习,类似于复盘学到哪一个位置怎么走是最有益的。这样的方法能够防止value based算法在感受野很小的地方来回往复选择，在无关痛痒的地区浪费更多的时间犹豫不定，依靠低概率的随机选择走出新的一步。<br>
PG通过观测信息(之前游戏中此位置所执行过的动作)选出一个action直接进行反向传播，当然出人意料的是他并没有误差，而是利用动作reward直接对选择行为的可能性进行增强和减弱，好的行为会被增加下一次被选中的概率，不好的行为会被减弱下次被选中的概率。

# 3、Actor-Critis
介绍完上面两种方法后相互之间优势互补能有更好的效果，下面图也可以说明然后该怎么做：
<img src="/img/191026image/pic-3.png" >
用流程图大致描述一下：
<img src="/img/191026image/actorcritis.jpg" >

[Q-learning详解](https://www.jianshu.com/p/277abf64e369)
[PG-theorem推导](https://blog.csdn.net/qq_30615903/article/details/80747380)